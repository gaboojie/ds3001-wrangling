{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
   "metadata": {
    "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
   },
   "source": [
    "# Assignment: Data Wrangling\n",
    "## `! git clone https://github.com/DS3001/wrangling`\n",
    "## Do Q2, and one of Q1 or Q3.\n",
    "\n",
    "I did Q1 and Q2.\n",
    "\n",
    "Gabriel Jackson (tbp8gx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
   "metadata": {
    "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
   },
   "source": [
    "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
    "\n",
    "  1. Read the abstract. What is this paper about?\n",
    "    \n",
    "    This paper sets forth a pattern to give better structure to datasets as a lot of time is spent cleaning data. This pattern includes how \"each variable is a column, each observation is a row, and each type of observational unit is a table.\" By creating structure and better conceptual design, these tidy datasets allow for easier cleaning (and even understanding) of data overall. \n",
    "      \n",
    "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
    "  \n",
    "    The \"tidy data standard\" is intended to make the process of data cleaning both easier and less time-consuming. By providing a \"philosophy of data\", the \"tidy data standard\" can be implemented.\n",
    "      \n",
    "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
    "\n",
    "    For the first sentence, this means that tidy datasets share common issues and (on the other hand) messy datasets tend to create their own set of problems. This means that it is easier to address the issues for tidy datasets and harder for messy ones (as messy ones don't tend to share the same issues). For the second sentence, it is trying to show how easy it is to mix-up variables and observations. A variable is fairly easy to understand; it contains 'all values measured on the same unit' (i.e. eye color). A observation can also be fairly easy to understand; it \"contains all values measured on the same unit across attributes\" (i.e. all values for the same person). In practice, however, it can be difficult to differentiate the two without extremely explicit definitions and understandings. \n",
    "\n",
    "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
    "    \n",
    "    a. Values are usually either numbers or strings and altogether form a dataset. Each value in a dataset belongs to a variable and an observation. (I like to think of one cell in a matrix that represents one variable for one observation. For instance, Jack's eye color being blue, where blue is the value.)\n",
    "    b. Variables 'contain all values that measure the same underlying attribute'. (I like to think about eye color or temperature.)\n",
    "    c. Observations 'contain all values measured on the same unit across attributes'. (I like to think about all values across the same instance, i.e. the same person.)\n",
    "\n",
    "\n",
    "  5. How is \"Tidy Data\" defined in section 2.3?\n",
    "    \n",
    "    'Tidy Data' can be used to describe data when 'each variable forms a column, each observation forms a row, and each type of observational unit forms a table.'\n",
    "    \n",
    "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
    "\n",
    "    Most common problems:\n",
    "    1. 'Column headers are values, not variable names.'\n",
    "    2. 'Multiple variables are stored in one column.'\n",
    "    3. 'Variables are stored in both rows and columns.'\n",
    "    4. 'Multiple types of observational units are stored in the same table.'\n",
    "    5. 'A single observational unit is stored in multiple tables.'\n",
    "    \n",
    "    The data in Table 4 is messy because the columns represent an income variable and should not be expressed across more than one column. Instead, an income variable should be defined across one column as well as a frequency variable (across one column) to represent how many times it happens.\n",
    "    \n",
    "    Melting a dataset (or stacking a dataset) refers to the need of turning columns into rows like in Table 4. \n",
    "\n",
    "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
    "\n",
    "    Table 11 is messy because there are values at the top that should be melted into a variable that Table 12 calls 'date'. Table 12b is tidy and \"molten\", because it not only uses the previously-mentioned date format, but also creates variables for 'tmax' and 'tmin' (which should have been variables and not values). \n",
    "\n",
    "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
    "\n",
    "    The \"chicken-and-egg\" problem says that \"if tidy data is only as useful as the tools that work with it, then tidy tools will be inextricably linked to tidy data.\" He suggests that we need to escape from this connection between tidy data and tidy tools and hopes that further work will be done in trying to properly conceptualize and define tools and topics used with data."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
    "\n",
    "*Note: All of my code is at the bottom in their respective sections (with questions labeled).*\n",
    "\n",
    "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
    "\n",
    "*To clean the 'Price' variable, I removed any commas from every value in the Price column. I then tried to cast each value to a numeric format (from a String format). I then summed any value that could not be coerced to a number, which totaled to 0. This means that I did not end up with any missing values. Additionally, I did see some outliers like 10000 as a number, but I didn't remove it, because it doesn't look like its out of place and could be a price for Airbnb. (They're so expensive.)*\n",
    "\n",
    "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
    "\n",
    "*To clean the 'Type' variable, I re-categorized certain unique values into two groups. The first group of 'Boat', 'Boatomg', 'Boating', and 'Sea Disaster' were categorized as 'Watercraft' as they are all water/water-vehicle specific. The second group of 'Questionable', 'Unconfirmed', 'Unverified', 'Invalid', and 'Under Investigation' were categorized as not-a-number as they all represent some sort of uncertainty in their type. This led to three main categories for the variable 'type': provoked, unprovoked, and watercraft (also NaN if considered a valid category and not an omission).*\n",
    "\n",
    "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
    "\n",
    "*I first downloaded the pretrial data on my local machine and made sure to include it in the .gitignore. To clean the data, I essentially replaced every 9 (which represents an error code) with not-a-number/np.nan. I then made sure to update the file with the cleaned data and delete my 'wasDefendantWasReleased' collection from memory. (However the pretrial data file is downloaded on my local machine and cannot be seen on GitHub).*\n",
    "\n",
    "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)\n",
    "\n",
    "*I first downloaded the pretrial data on my local machine and made sure to include it in the .gitignore. To clean the data, I had to look at 'SentenceTypeAllChargesAtConvictionInContactEvent' and determine that if the code for a value was a '4', it must be False (numerically 0) as it represents a dismissed case. Additionally, if the code in 'SentenceTypeAllChargesAtConvictionInContactEvent' was a 9, this represents uncertainty and should be not-a-number, so it was replaced with 'np.nan'. I then masked according to the 'sentenceType' variable's code (if it was '4' or '9') to its proper value in the 'imposedSentence' variable. This changed the 9053 missing values to only 274, which is a lot better. I then updated 'ImposedSentenceAllChargeInContactEvent' to reflect the data (however the pretrial data file is downloaded on my local machine and cannot be seen on GitHub). (I also deleted the 'imposedSentence' and 'sentenceType' from memory.)*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c4504e3f2bed954"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Include packages and open file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:54:29.755205Z",
     "start_time": "2024-02-04T18:54:28.326582Z"
    }
   },
   "id": "d7be5b757aea8e96",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Question 2.1: Clean the Price variable\n",
    "\n",
    "df = pd.read_csv('./data/airbnb_hw.csv', low_memory=False)\n",
    "price = df['Price'] \n",
    "price = price.str.replace(',', '') # Remove commas as the values are strings and some have commas\n",
    "# print(price.unique(), '\\n') # String formatted, needs to be converted to numbers\n",
    "price = pd.to_numeric(price, errors='coerce') # Coerce the conversion from string to numbers \n",
    "print(price.unique(), '\\n') # Print number-formatted price\n",
    "print('Number of values that could not be coerced: ', sum(price.isnull())) # Number of values that were cleaned\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "608b1e50c2f75850",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Question 2.2: Clean the Type variable\n",
    "\n",
    "df = pd.read_csv('./data/sharks.csv', low_memory=False)\n",
    "type = df['Type']\n",
    "# print(type.unique(), '\\n') # I noticed several issues for unique types, i.e. water-specific and uncertainty categories\n",
    "\n",
    "# Recategorize water-specific issues to 'watercraft'\n",
    "type = type.replace(['Boat', 'Boatomg', 'Boating', 'Sea Disaster'], 'Watercraft')\n",
    "\n",
    "# Recategorize uncertainty categories to nan\n",
    "type = type.replace(['Questionable', 'Unconfirmed', 'Unverified', 'Invalid', 'Under investigation'], np.nan)\n",
    "\n",
    "df['Type'] = type # Update type with cleaned type\n",
    "print(type.value_counts(), '\\n') # Print values and their counts\n",
    "del type # Delete type from memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41076b36a05f0899",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Question 2.3: Clean the 'WhetherDefendantWasReleasedPretrial' variable \n",
    "\n",
    "# I downloaded the file locally as it is 50 mb and I did not upload it to GitHub. (It is git ignored.)\n",
    "df = pd.read_csv('./data/pretrialdataproject.csv', low_memory=False)\n",
    "\n",
    "wasDefendantReleased = df['WhetherDefendantWasReleasedPretrial']\n",
    "# print(wasDefendantReleased.unique(), '\\n') # I know that 9 represents not-a-number so it should be np.nan\n",
    "wasDefendantReleased = wasDefendantReleased.replace(9, np.nan)\n",
    "print(\"Number of not-a-number: \", sum(wasDefendantReleased.isnull())) # Print how much values were replaced with not-a-number, originally 31\n",
    "# print(wasDefendantReleased.unique(), '\\n')\n",
    "df['WhetherDefendantWasReleasedPretrial'] = wasDefendantReleased # Update file\n",
    "print(wasDefendantReleased.value_counts(), '\\n') # Print values and their counts\n",
    "del wasDefendantReleased # Remove from memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc299224ce4b94cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precleaned: 9053 \n",
      "\n",
      "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
      "ImposedSentenceAllChargeInContactEvent                                      \n",
      "False                                             8720  4299  914     0    0\n",
      "True                                                 0     0    0  8779  274 \n",
      "\n",
      "After cleaning: 274 \n",
      "\n",
      "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
      "ImposedSentenceAllChargeInContactEvent                                      \n",
      "False                                             8720  4299  914  8779    0\n",
      "True                                                 0     0    0     0  274 \n"
     ]
    }
   ],
   "source": [
    "# Question 2.4: Clean the 'ImposedSentenceAllChargeInContactEvent' variable\n",
    "\n",
    "# I downloaded the file locally as it is 50 mb and I did not upload it to GitHub. (It is git ignored.)\n",
    "df = pd.read_csv('./data/pretrialdataproject.csv', low_memory=False)\n",
    "\n",
    "imposedSentence = df['ImposedSentenceAllChargeInContactEvent']\n",
    "sentenceType = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
    "\n",
    "# Imposed sentence needs to be in numeric form\n",
    "imposedSentence = pd.to_numeric(imposedSentence, errors='coerce')\n",
    "print('Precleaned:', np.sum(imposedSentence.isnull()), '\\n') # 9053 missing\n",
    "print(pd.crosstab(imposedSentence.isnull(), sentenceType), '\\n')\n",
    "\n",
    "imposedSentence = imposedSentence.mask(sentenceType == 4, 0) # Code 4 means case dismissed (False)\n",
    "imposedSentence = imposedSentence.mask(sentenceType == 9, np.nan) # Code 9 means not-a-number/uncertain\n",
    "print('After cleaning:', np.sum(imposedSentence.isnull()), '\\n')\n",
    "print(pd.crosstab(imposedSentence.isnull(), sentenceType), '\\n')\n",
    "\n",
    "# print(imposedSentence.unique(), '\\n')\n",
    "# print(sentenceType.unique(), '\\n')\n",
    "\n",
    "df['ImposedSentenceAllChargeInContactEvent'] = imposedSentence # Replace data with cleaned version\n",
    "del imposedSentence, sentenceType # Delete temporary length/type variables"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T19:07:36.306084Z",
     "start_time": "2024-02-04T19:07:31.249917Z"
    }
   },
   "id": "c3e0e8239594ee13",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fa29bc7ec58a0af"
  },
  {
   "cell_type": "markdown",
   "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
   "metadata": {
    "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
   },
   "source": [
    "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
    "\n",
    "1. How did the most recent US Census gather data on race?\n",
    "\n",
    "\n",
    "\n",
    "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
    "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
    "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
    "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
    "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
